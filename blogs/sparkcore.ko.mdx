---
id: 1251
date: 2023-10-12
tags: ["spark"]
title: Spark core
slug: sparkcore
seq: 4
type: interest
language: kr
featureImage: /images/interest/spark-big1.png
thumb: /images/interest/spark.png
github:
demo:
videoSourceURL:
videoTitle:
excerpt: 데이터 파이프라인을 관리하는 툴로 정교한 스케쥴기반의 프로세스를 통제한다.
---

### Contents

## spark rdd 기본

    <TabssCompare
          data={[
            {
              id: "spark_pyspark",
              label: "pyspark",
              script: "cat /home/yknam/pythonfiles/pyspark/sparkRdd.py",
              content: (
                <SearchShow
                  id="spark_1"
                  script="cat /home/yknam/pythonfiles/pyspark/sparkRdd.py"
                  script1="/home/yknam/pythonfiles/pyspark/sparkRdd.py"
                  compare="pyspark"
                />
              ),
            },
            {
              id: "spark_scala",
              label: "scala",
              script:
                "cat /home/yknam/scalaproject/scala_spark/src/main/scala/SparkRdd.scala",
              content: (
                <SearchShow
                  id="spark_scala2"
                  script="cat /home/yknam/scalaproject/scala_spark/src/main/scala/SparkRdd.scala"
                  script1="spark-submit --class com.yknam.SparkRdd /home/yknam/scalaproject/scala_spark/target/scala-2.12/scala_spark_2.12-0.1.0-SNAPSHOT.jar"
                  compoare="scala"
                />
              ),
            },
          ]}
        ></TabssCompare>

## spark rdd to DF

    <SearchShow
          id="spark_2"
          script="cat /home/yknam/pythonfiles/pyspark/sparkRddToDF.py"
          script1="/home/yknam/pythonfiles/pyspark/sparkRddToDF.py"
        />

## spark DataFrame

    <SearchShow
          id="spark_3"
          script="cat /home/yknam/pythonfiles/pyspark/sparkDataframe.py"
          script1="/home/yknam/pythonfiles/pyspark/sparkDataframe.py"

        />

## spark rdd WordCount

    <SearchShow
          id="spark_4"
          script="cat /home/yknam/pythonfiles/pyspark/sparkRddWordCount.py | grep '##'"
          script1="/home/yknam/pythonfiles/pyspark/sparkRddWordCount.py"

        />

## master설정

    <div style={{ float: "right", marginLeft: "20px" }}>
          <img src="/images/interest/spark-master.png" width="400" alt="master" />
          </div>

- **driver only**: worker없이 driver로만 실행됨 local (코어갯수지정)
- **standalone**: spark://localhost:7077로 지정. spark만으로 클러스터를 구성
- **multinode**: resource manager와 node manager를 yarn에 의존한다.

```
- driver only: spark.master("local[1]")
- stand alone: spark.master("spark://localhost:7077")
- multinode: spark.master("yarn")
```

    <div style={{ clear: "both" }} />

## shared variable
     <div className="flex w-full justify-center items-center">
           <img
              src="/images/interest/sharedvariable.png"
              alt="sharedvariable"
              width="50%"
            />
          </div>
### broadcast

    <div style={{ float: "right", marginLeft: "20px" }}>
          <img
              src="/images/interest/sparkcore-broadcast.png"
              width="400"
              alt="broadcastjoin"
            />
          </div>
           

> 브로드캐스트변수는 파일 사이즈가 작을 경우 driver에서 각 work로 복사하여 사용한다.

    <div style={{ clear: "both" }} />
    <SearchShow
          id="spark_6"
          script="cat /home/yknam/pythonfiles/pyspark/sparkRddBroadcast.py"
          script1="/home/yknam/pythonfiles/pyspark/sparkRddBroadcast.py"
        />
   

### accumulator


    <div style={{ float: "right", marginLeft: "20px" }}>
          <img
              src="/images/interest/accumulator.png"
              width="400"
              alt="accumulator"
            />
          </div>

> accumulator변수는 각worker에서 공유되는 변수로 driver에서 변수값이 통합관리된다.

    <div style={{ clear: "both" }} />
    <SearchShow
          id="spark_5"
          script="cat /home/yknam/pythonfiles/pyspark/sparkRddAccumulator.py"
          script1="/home/yknam/pythonfiles/pyspark/sparkRddAccumulator.py" />

## broadcast join vs shuffle join

    <div style={{ float: "right", marginLeft: "20px" }}>
          <img
              src="/images/interest/sparkcore-shufflejoin.png"
              width="400"
              alt="shufflejoin"
            />
          </div>

> broadkcast hash join : map side only join 이라고도 함. 각각의 worker 노드내에서 조인이 이루어지고 shuffle이 발생되지 않는다.
> 조인되는 데이터중 작은 데이터셋이 Driver에서 각각의 worker node로 복사되어 조인이 이루어진다.

    <div style={{ clear: "both" }} />
    <div style={{ float: "right", marginLeft: "20px" }}>
          <img src="/images/interest/shufflejoin.png" width="400" alt="shufflejoin1" />
          </div>

> shuffle sort join: worker간에 shuffle이 발생하여 조인이 일어나는 경우.
> 조인되는 데이터셋이 둘다 클경우에는

    <div style={{ clear: "both" }} />
