---
id: 125
date: 2023-09-12
tags: ["filebeat", "elk"]
title: Filebeat
slug: filebeat
seq: 4
type: interest
language: kr
featureImage: /images/interest/filebeat.png
thumb: /images/interest/filebeat_icon.jpg
github:
demo: http://imcmaster.iptime.org:5601/app/dashboards#/view/79ffd6e0-faa0-11e6-947f-177f697178b8-ecs?_g=(filters:!(),refreshInterval:(pause:!t,value:0),time:(from:'2023-09-12T02:00:00.000Z',to:now))
videoSourceURL:
videoTitle:
excerpt: 간편하게 메트릭 데이터를 수집.
---

## Filebeat 설정

> filebeat.yml파일에 input, output을 설정하여 실행한다. 하나씩만 설정, 실행할 수 있다.
>
> input 데이터 소스로 nginx 로그를 수집하는 예이다. type->log, enable->true,path->/var/log/nginx/access.log로 설정한다.
>
> output 으로 elatic을 선택했다. logstash를 거쳐 filtering해서 elastic으로 전송할 경우에는 elastic을 주석처리하고 logstash를 활성화하면된다.

```yaml
# ============================== Filebeat inputs ===============================

filebeat.inputs:

- type: log

  # Unique ID among all inputs, an ID is required.
  id: my-filestream-id

  # Change to true to enable this input configuration.
  enabled: true

  # Paths that should be crawled and fetched. Glob based paths.
  paths:
    - /var/log/nginx/access.log

....
....
....

# ================================== Outputs ===================================

# ---------------------------- Elasticsearch Output ----------------------------
output.elasticsearch:
  # Array of hosts to connect to.
  hosts: ["localhost:9200"]

  # Protocol - either `http` (default) or `https`.
  #protocol: "https"

  # Authentication credentials - either API key or username/password.
  #api_key: "id:api_key"
  #username: "elastic"
  #password: "changeme"

# ------------------------------ Logstash Output -------------------------------
output.logstash:
  # The Logstash hosts
  hosts: ["localhost:5044"]

  # Optional SSL. By default is off.
  # List of root certificates for HTTPS server verifications
  #ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]

  # Certificate for SSL client authentication
  #ssl.certificate: "/etc/pki/client/cert.pem"

  # Client Certificate Key
  #ssl.key: "/etc/pki/client/cert.key"

```

### filebeat 실행

> filebeat 실행command : ./filebeat
>
> filebeat는 한번 실행한 데이터는 다시 읽지 않기 때문에 새로 데이터를 읽으려면 data디렉토리를 삭제한 후 실행해야함

### filebeat -> kafka -> logstash -> elasticsearch

<img src="/images/interest/filebeat-pipeline.png" width="100%" alt="pipeline" />

1.fielbeat.yml

> filebeat.yml에 kafka output을 추가한다.

```js
# ------------------------------ Kafka Output -------------------------------
output.kafka:
  # initial brokers for reading cluster metadata
  hosts: ["winubuntu:9092","namubuntu:9092"]

  # message topic selection + partitioning
  topic: "filebeat"
  partition.round_robin:
    reachable_only: false

  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000
```

2. ./filebeat 실행
3. kafka 실행

- topic 생성: filebeat
- producer 실행: filebeat

4. logstash실행
   > logstash conf파일을 생성한다.
   > input: kafka, output:elasticsearch
   > logstash를 실행하면 kafka에서 데이터를 가져와 elasticsearch로 전송한다.

```json
input {
    kafka {
        bootstrap_servers =>  "winubuntu:9092, namubuntu:9092"
        topics => ["filebeat"]
    }
}

filter {
}

output {
    stdout {
        codec => rubydebug
    }
    elasticsearch {
        hosts => "http://localhost:9200"
        index => "kafka-filebeat"
        document_type => "_doc"
    }
}
```

5. elasticsearch

<SearchShow
  script="cat /home/yknam/pythonfiles/pyspark/sparkRddToDF.py"
  script1="ssh yknam@namubuntu curl localhost:9200/filebeat*/_search"
  port="7878"
/>
