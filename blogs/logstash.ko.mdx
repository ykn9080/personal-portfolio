---
id: 125
date: 2023-09-30
tags: ["elastic", "logstash", "elk"]
title: Logstash
slug: logstash
seq: 4
type: interest
language: kr
featureImage: /images/interest/logstash.png
thumb: /images/interest/logstash_icon.png
github:
demo: http://imcmaster.iptime.org:5601/app/dev_tools#/console
videoSourceURL:
videoTitle:
excerpt: 데이터를 수집하여 엘라스틱 또는 다른 파이프라인으로 전송.
---

## Logstash 구성

<img src="/images/interest/logstash_pipeline1.png" alt="logstash-pipeline" />

> 기본 구성 conf 파일은 input,filter,output으로 구성된다.

```bash
input {}
filter {}
output {}
```

> 실행은 bin directory의 logstash를 구동하며 -f 로 conf 파일을 지정한다.

```
bin/logstash -f conf/example.conf
```

## Input설정

```
tcp {
  port => 12345
  codec => json
}
beats {
  port => 5044
}
 file {
  id => "my_plugin_id"
  path => "/var/log/*"
  exclude => "*.gz"
}
jdbc {
  jdbc_driver_library => "/home/yknam/sandbox/logstash/mysql-connector-java-8.0.11.jar"
  jdbc_driver_class => "com.mysql.jdbc.Driver"
  jdbc_connection_string => "jdbc:mysql://winubuntu:3336/sakila"
  jdbc_user => "user"
  jdbc_password => "pass"
  statement => "SELECT id, mycolumn1, mycolumn2 FROM my_table WHERE id > :sql_last_value",
  #statement => "CALL fetch_my_data(:sql_last_value, :offset, :size)",
  jdbc_paging_enabled => true,
  jdbc_paging_mode => "explicit",
  jdbc_page_size => 100000
}

```

## filter설정

```bash
#data
55.3.244.1 GET /index.html 15824 0.043
#grok
%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}
#result
{
  "duration": "0.043",
  "request": "/index.html",
  "method": "GET",
  "bytes": "15824",
  "client": "55.3.244.1"
}

```

#### grok pattern

- pattern 형태: `%{IP: ip}`
- sample: 2.3.4.5 => `%{IP:ip} => {ip:"2.3.4.5"}`
- pattern유형
  IP  
  GREEDYDATA : 모든 유형의 데이터
  SPACE  
  WORD  
  UUID  
  MAC  
  EMAILADDRESS
  NUMBER  
  INT  
  QUOTEDSTRING

## Output설정

```bash
file {
   path => ...
   codec => line { format => "custom format: %{message}"}
}
elasticsearch {
  hosts => ["http://localhost:9200"]
  index => "%{[@metadata][beat]}-%{[@metadata][version]}"
}
stdout { codec => json }
syslog {
  id => "my_plugin_id"
}
kafka {
  codec => json
  topic_id => "mytopic"
}
```

### filebeat -> kafka -> logstash -> elasticsearch

<img src="/images/interest/filebeat-pipeline.png" width="100%" alt="pipeline" />

1.fielbeat.yml

> filebeat.yml에 kafka output을 추가한다.

```js
# ------------------------------ Kafka Output -------------------------------
output.kafka:
  # initial brokers for reading cluster metadata
  hosts: ["winubuntu:9092","namubuntu:9092"]

  # message topic selection + partitioning
  topic: "filebeat"
  partition.round_robin:
    reachable_only: false

  required_acks: 1
  compression: gzip
  max_message_bytes: 1000000
```

2. ./filebeat 실행
3. kafka 실행

- topic 생성: filebeat
- producer 실행: filebeat

4. logstash실행
   > logstash conf파일을 생성한다.
   > input: kafka, output:elasticsearch
   > logstash를 실행하면 kafka에서 데이터를 가져와 elasticsearch로 전송한다.

```json
input {
    kafka {
        bootstrap_servers =>  "winubuntu:9092, namubuntu:9092"
        topics => ["filebeat"]
    }
}

filter {
}

output {
    stdout {
        codec => rubydebug
    }
    elasticsearch {
        hosts => "http://localhost:9200"
        index => "kafka-filebeat"
        document_type => "_doc"
    }
}
```

5. elasticsearch

<SearchShow
  script="cat /home/yknam/pythonfiles/pyspark/sparkRddToDF.py"
  script1="ssh yknam@namubuntu curl localhost:9200/filebeat*/_search"
  port="7878"
/>

#### jdbc

<div style={{ float: "right", marginLeft: "20px" }}>
  <img
    src="/images/interest/sparkcore-shuffle.png"
    width="300"
    alt="shufflejoin"
  />
</div>

> logstash로 mysql데이터를 query해서 화면에 출력함
> input: mysql, filter:none, output:stdout

<div style={{ clear: "both" }} />
<SearchShow
  script="ssh yknam@namubuntu cat /home/yknam/sandbox/logstash/conf/jdbcstdout.conf"
  script1="ssh yknam@namubuntu  rm -rf /home/yknam/sandbox/logstash/logs3;/home/yknam/sandbox/logstash/bin/logstash -f /home/yknam/sandbox/logstash/conf/jdbcstdout.conf --path.data=logs3"
  port="7878"
/>

#### 유용한 tip

- 개발시 자동 reload되도록 config/logstash.yml 수정

```
config.reload.automatic : true
```
